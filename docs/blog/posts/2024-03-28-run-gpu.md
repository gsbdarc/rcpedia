---
date:
 created: 2024-03-28
 updated: 2024-11-16
categories:
   - Machine Learning
   - GPU
---

# Train Machine Learning Models on GPU

If you are running machine learning or deep learning algorithms, you might benefit from using a GPU. Currently, the Yens have three GPU nodes, each equipped with 4 GPUs. However, there are also other GPU resources available to the Stanford research computing community. This page will focus on GPU resources outside of Yens. If you’d like to learn how to run jobs on Yen’s GPU nodes, please visit [this page](/blog/2024/04/25/run-jobs-on-yens-gpu-node/){:target="_blank"}.

<!-- more -->

## Sherlock HPC

Sherlock also has over 700+ GPUs that we can take advantage of when training machine learning or deep learning models. See [this page](/_policies/sherlock){:target="_blank"} for getting started using Sherlock.

Login to the [Sherlock HPC](https://www.sherlock.stanford.edu/docs/overview/introduction/){:target="_blank"} system:

```title="Terminal Command"
ssh <$USER>@sherlock.stanford.edu
```
Enter your SUNet ID and authenticate with Duo to complete the login process.

### Install Miniconda

To install miniconda in `/oak/stanford/projects/<your-lab>/<$USER>/miniconda`, use this shell script named `install_miniconda.sh`:

```bash linenums="1" title="install_miniconda.sh"
#!/bin/sh

wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh

chmod +x Miniconda3-latest-Linux-x86_64.sh

./Miniconda3-latest-Linux-x86_64.sh

conda config --set auto_activate_base false
```

You can save this file to your **home** directory and call it `install_miniconda.sh` for example. When you run the script from your home directory, it will ask to provide a path where to install miniconda.

Make sure to paste your **Oak path** instead of the default home installation (for example, `/oak/stanford/projects/<your-lab>/<$USER>/miniconda`). We want to avoid creating multiple conda environments in the home directory because, with machine learning, data science, and deep learning packages, it is possible to run out of space after setting up just a few environments. Therefore, use Oak or another path where you have large amount of space.

See [this page](https://uit.stanford.edu/service/oak-storage/){:target="_blank"} to learn more about Oak and to purchase space for your lab.

To run the script from home directory:

```title="Terminal Command"
sh install_miniconda.sh
```

After miniconda is installed, add the path to conda bin to your bash profile so that `conda` and `python` executables are found:

```bash linenums="1" title=".bash_profile"

# Add path to miniconda
export PATH=$PATH:/oak/stanford/projects/<your-lab>/<$USER>/miniconda3/bin
```
Source bash profile to apply changes:

```title="Terminal Command"
source ~/.bash_profile
```
### Create Conda Environment

To install deep learning python packages to work on GPU, we need to have CUDA loaded.
Load cuda module:

```title="Terminal Command"
module load cuda/11.0.3
```
Make sure `conda` is found and is the one that you want:

```title="Terminal Command"
which conda
```
should return:

```{ .yaml .no-copy title="Terminal Output" }
/oak/stanford/projects/<your-lab>/<$USER>/miniconda3/bin/conda
```

Create a new conda environment and install packages using `conda`. Note that `pip` and `conda` may cause conflicts in this case, so we will use only `conda` to properly manage the packages.

```title="Terminal Command"
conda create -n tf-gpu python=3.8
```
Activate the conda environemnt:

```title="Terminal Command"
source activate tf-gpu
```

Then, install the following packages:

```title="Terminal Command"
conda install tensorflow-gpu keras pandas scikit-learn
```

!!! Important
    `pip` does not properly manage GPU-based packages for `keras` and `tensorflow`, so we must use conda install to install the GPU-based packages we need.

Afterward, we can activate this conda environment in the Slurm submission script.

### Keras Example

We will run this python script on the Sherlock GPU to train a simple MNIST convnet. Save the following to `mnist.py`

```python linenums="1" title="Python"
import numpy as np
from tensorflow import keras
from tensorflow.keras import layers

# Model / data parameters
num_classes = 10
input_shape = (28, 28, 1)

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

# Make sure images have shape (28, 28, 1)
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print("x_train shape:", x_train.shape)
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

# build the model
model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv2D(32, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Conv2D(64, kernel_size=(3, 3), activation="relu"),
        layers.MaxPooling2D(pool_size=(2, 2)),
        layers.Flatten(),
        layers.Dropout(0.5),
        layers.Dense(num_classes, activation="softmax"),
    ]
)

print(model.summary())

# train the model
batch_size = 128
epochs = 15

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)

# Evaluate the trained model
score = model.evaluate(x_test, y_test, verbose=0)
print("Test loss:", score[0])
print("Test accuracy:", score[1])
```

### Run Slurm Script

The submission script `keras-gpu.slurm` looks like:

```bash linenums="1" title="keras-gpu.slurm"
#!/bin/bash

# Example slurm script to run Keras DL models on Sherlock GPU

#SBATCH -J train-gpu
#SBATCH -p gpu
#SBATCH -c 20
#SBATCH -N 1
#SBATCH -t 1-
#SBATCH -G 1
#SBATCH -o train-gpu-%j.out
#SBATCH --mail-type=ALL
#SBATCH --mail-user=your_email@stanford.edu

source $HOME/.bash_profile
module load cuda/11.0.3
source activate tf-gpu

python mnist.py
```

This script is asking for one GPU on `gpu` partition and 20 CPU cores on one node for 1 day.

Submit the job to the `gpu` partition with:

```title="Terminal Command"
sbatch keras-gpu.slurm
```

Monitor your job:

```title="Terminal Command"
squeue -u $USER
```

You should see something like:

```{ .yaml .no-copy title="Terminal Output" }
       JOBID PARTITION     NAME     USER   ST     TIME  NODES NODELIST(REASON)
    20372833       gpu train-gp nrapstin    R     0:05      1 sh03-12n07
```

Once the job is running, connect to the node and monitor your GPU utilization:

```title="Terminal Command"
ssh sh03-12n07
```

Once you connect to the GPU node, load the cuda module there and monitor GPU utilization while the job is running:

```title="Terminal Command"
module load cuda/11.0.3
watch nvidia-smi
```

You should see that the GPU is being utilized (under GPU-Util column):

```{ .yaml .no-copy title="Terminal Output" }
Every 2.0s: nvidia-smi
Tue Nov 15 13:43:04 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  On   | 00000000:C4:00.0 Off |                  N/A |
|  0%   44C    P2   114W / 260W |  10775MiB / 11264MiB |     40%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      4876      C   python                          10772MiB |
+-----------------------------------------------------------------------------+
```

Once the job is done, look at the output file:

```title="Terminal Command"
cat train-gpu*.out
```

The output should look similar to this:

```{ .yaml .no-copy title="Terminal Output" }
/etc/profile.d/z99_srcc.sh: line 183: SHERLOCK: readonly variable
/etc/profile.d/z99_srcc.sh: line 319: PI_SCRATCH: readonly variable
2021-03-15 11:52:52.231386: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
2021-03-15 12:08:05.866384: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-15 12:08:05.907107: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-03-15 12:08:06.000808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:c4:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-03-15 12:08:06.000917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-03-15 12:08:08.793249: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-03-15 12:08:08.793435: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-03-15 12:08:12.168920: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-15 12:08:12.517624: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-15 12:08:13.562000: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-15 12:08:13.966972: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-03-15 12:08:16.409172: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-03-15 12:08:16.412824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-15 12:08:16.430127: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-03-15 12:08:16.433601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:c4:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.62GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2021-03-15 12:08:16.433698: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-03-15 12:08:16.433743: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-03-15 12:08:16.433772: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-03-15 12:08:16.433799: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-03-15 12:08:16.433826: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-03-15 12:08:16.433875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-03-15 12:08:16.433904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-03-15 12:08:16.433931: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-03-15 12:08:16.437218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-03-15 12:08:16.437288: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-03-15 12:08:22.623609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-03-15 12:08:22.623693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-03-15 12:08:22.623711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-03-15 12:08:22.626838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10073 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:c4:00.0, compute capability: 7.5)
2021-03-15 12:08:22.668291: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-03-15 12:08:23.407609: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-03-15 12:08:23.531488: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2500025000 Hz
2021-03-15 12:08:24.419584: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-03-15 12:08:25.272119: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 26, 26, 32)        320
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0
_________________________________________________________________
flatten (Flatten)            (None, 1600)              0
_________________________________________________________________
dropout (Dropout)            (None, 1600)              0
_________________________________________________________________
dense (Dense)                (None, 10)                16010
=================================================================
Total params: 34,826
Trainable params: 34,826
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/15
422/422 [==============================] - 6s 5ms/step - loss: 0.7651 - accuracy: 0.7647 - val_loss: 0.0860 - val_accuracy: 0.9765
Epoch 2/15
422/422 [==============================] - 1s 2ms/step - loss: 0.1198 - accuracy: 0.9634 - val_loss: 0.0576 - val_accuracy: 0.9842
Epoch 3/15
422/422 [==============================] - 1s 2ms/step - loss: 0.0892 - accuracy: 0.9728 - val_loss: 0.0464 - val_accuracy: 0.9875
Epoch 4/15
422/422 [==============================] - 1s 2ms/step - loss: 0.0716 - accuracy: 0.9773 - val_loss: 0.0420 - val_accuracy: 0.9878
Epoch 5/15
422/422 [==============================] - 1s 3ms/step - loss: 0.0609 - accuracy: 0.9815 - val_loss: 0.0380 - val_accuracy: 0.9903
Epoch 6/15
422/422 [==============================] - 1s 3ms/step - loss: 0.0585 - accuracy: 0.9819 - val_loss: 0.0390 - val_accuracy: 0.9888
Epoch 7/15
422/422 [==============================] - 1s 2ms/step - loss: 0.0474 - accuracy: 0.9853 - val_loss: 0.0394 - val_accuracy: 0.9885
Epoch 8/15
422/422 [==============================] - 1s 3ms/step - loss: 0.0493 - accuracy: 0.9850 - val_loss: 0.0334 - val_accuracy: 0.9907
Epoch 9/15
422/422 [==============================] - 1s 3ms/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.0322 - val_accuracy: 0.9912
Epoch 10/15
422/422 [==============================] - 1s 3ms/step - loss: 0.0364 - accuracy: 0.9883 - val_loss: 0.0318 - val_accuracy: 0.9918
Epoch 11/15
422/422 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.9883 - val_loss: 0.0297 - val_accuracy: 0.9913
Epoch 12/15
422/422 [==============================] - 1s 2ms/step - loss: 0.0372 - accuracy: 0.9881 - val_loss: 0.0284 - val_accuracy: 0.9910
Epoch 13/15
422/422 [==============================] - 1s 3ms/step - loss: 0.0344 - accuracy: 0.9891 - val_loss: 0.0305 - val_accuracy: 0.9913
Epoch 14/15
422/422 [==============================] - 3s 7ms/step - loss: 0.0324 - accuracy: 0.9896 - val_loss: 0.0284 - val_accuracy: 0.9910
Epoch 15/15
422/422 [==============================] - 1s 3ms/step - loss: 0.0299 - accuracy: 0.9899 - val_loss: 0.0277 - val_accuracy: 0.9923
Test loss: 0.02463490329682827
Test accuracy: 0.9919000267982483
```

If you see errors or warnings related to GPU and CUDA libraries not being found, it is likely that `keras` and `tensorflow` were not installed correctly. Make sure to load the CUDA module, then try removing and reinstalling the conda environment.

## Google Colab

[Google Colab](https://colab.research.google.com/){:target="_blank"} allows you to run Jupyter notebooks in the cloud, using either a CPU or adding GPU and TPU accelerators. We will use the free Colab tier, but for longer training jobs or access to better GPUs (e.g., T4, P100, or V100), the [paid Colab Pro or Colab Pro+ option](https://colab.research.google.com/signup){:target="_blank"} may be a better choice. Navigate to [Colab website](https://colab.research.google.com/notebooks/gpu.ipynb){:target="_blank"} and check out an example Jupyter notebook that uses a GPU for machine learning training. 

!!! Note
    In the free version of Colab, access to expensive resources like GPUs is heavily restricted and depends on availability as well as your usage patterns. Additionally, notebooks can run for a maximum of 12 hours in a single session, and the types of GPUs and TPUs available in Colab may vary over time. Paid plans offer increased compute availability based on your compute unit balance.

One advantage of using Colab is that common machine learning packages like `keras`, `tensorflow`, and `xgboost` come pre-installed with GPU support, so there is no need for python environment setup as required on on-premise systems like Sherlock. Colab also integrates seamlessly with Google Drive, allowing you to upload your data and Jupyter notebooks to Drive for use in the Colab environment. 

### Start a Colab Notebook

Follow these steps to open your notebook in Colab:

1. Upload your notebook to Google Drive
2. Right-click on the notebook file
3. Select `Open with` from the context menu
4. Choose `Google Colaboratory`

If Google Colaboratory does not appear in the drop-down menu:

1. Click `Connect more apps` at the bottom of the `Open with` menu
2. Search for `Colaboratory` in the app store
3. Select `Colaboratory` and connect it to your Google account

Once connected, Colab will be available as an option in the `Open with` menu for future use. This only needs to be done once. After that, Colab will be available as an option for opening notebooks in the future.

If you need to create a notebook from scratch:

1. Go to your Google Drive
2. Sign in with your Google account, if you’re not already signed in.
3. Click `+ New > More > Google Colaboratory` to start a new notebook
4. Save the notebook to Google Drive by clicking `File > Save` and selecting a location in your Drive

### Switch to Using a GPU

By default, notebooks in Colab run on the CPU. To leverage faster computations, you can switch to a GPU or TPU. Follow these steps to select a GPU accelerator:

1. Go to `Edit` -> `Notebook settings` in the Colab interface.
2. From the Hardware accelerator drop-down menu, select `GPU` (or `TPU` if your task is TPU-optimized).
3. Click `Save`.

Now your notebook will run on the GPU. GPU-accelerated execution allows faster training and inference for deep learning models. 

!!! Tip
    If the notebook is idle for an extended period, it may disconnect from Colab. Reconnect by refreshing the page and remounting resources (like Google Drive) if needed. When the session times out or disconnects, autosave will retain your most recent changes. However, it’s a good practice to manually save your notebook periodically to ensure no work is lost.

### Access Data from Google Drive

You can access data stored in your Google Drive from Colab by mounting it. This allows you to easily upload, store, and load files directly from Google Drive. To do this, run the following code in a cell within a Colab notebook (e.g., `colab_test.ipynb`):

```python title="colab_test.ipynb"
# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')
```
When prompted, click `Connect to Google Drive`, then choose your Google account to authorize access for your project. Once mounted, your Google Drive is accessible at `/content/drive`. For example, if you have a folder named `ml_project` in your Google Drive with a subdirectory `data`, you can reference it in Colab like this:

```python title="colab_test.ipynb"
# Define input data path
path_to_data = '/content/drive/MyDrive/ml_project/data'
```

### Run the Colab Notebook

Now we are ready to run the notebook, cell by cell. If we copy and paste the code from `mnist.py`above into a cell and execute it, the output should look similar to this:

```{ .yaml .no-copy title="Notebook Output" }
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Model: "sequential_1"
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓
┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩
│ conv2d_2 (Conv2D)                    │ (None, 26, 26, 32)          │             320 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_2 (MaxPooling2D)       │ (None, 13, 13, 32)          │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ conv2d_3 (Conv2D)                    │ (None, 11, 11, 64)          │          18,496 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ max_pooling2d_3 (MaxPooling2D)       │ (None, 5, 5, 64)            │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ flatten_1 (Flatten)                  │ (None, 1600)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dropout_1 (Dropout)                  │ (None, 1600)                │               0 │
├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤
│ dense_1 (Dense)                      │ (None, 10)                  │          16,010 │
└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘
 Total params: 34,826 (136.04 KB)
 Trainable params: 34,826 (136.04 KB)
 Non-trainable params: 0 (0.00 B)
None
Epoch 1/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 48s 112ms/step - accuracy: 0.7578 - loss: 0.7626 - val_accuracy: 0.9785 - val_loss: 0.0816
Epoch 2/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 79s 107ms/step - accuracy: 0.9645 - loss: 0.1147 - val_accuracy: 0.9817 - val_loss: 0.0633
Epoch 3/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 48s 114ms/step - accuracy: 0.9725 - loss: 0.0895 - val_accuracy: 0.9870 - val_loss: 0.0486
Epoch 4/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 79s 106ms/step - accuracy: 0.9780 - loss: 0.0712 - val_accuracy: 0.9900 - val_loss: 0.0414
Epoch 5/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 82s 106ms/step - accuracy: 0.9802 - loss: 0.0627 - val_accuracy: 0.9897 - val_loss: 0.0388
Epoch 6/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 80s 102ms/step - accuracy: 0.9828 - loss: 0.0590 - val_accuracy: 0.9910 - val_loss: 0.0344
Epoch 7/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 44s 105ms/step - accuracy: 0.9838 - loss: 0.0512 - val_accuracy: 0.9907 - val_loss: 0.0322
Epoch 8/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 83s 107ms/step - accuracy: 0.9844 - loss: 0.0507 - val_accuracy: 0.9915 - val_loss: 0.0301
Epoch 9/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 47s 112ms/step - accuracy: 0.9871 - loss: 0.0419 - val_accuracy: 0.9917 - val_loss: 0.0307
Epoch 10/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 45s 106ms/step - accuracy: 0.9866 - loss: 0.0428 - val_accuracy: 0.9918 - val_loss: 0.0293
Epoch 11/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 44s 105ms/step - accuracy: 0.9863 - loss: 0.0432 - val_accuracy: 0.9923 - val_loss: 0.0298
Epoch 12/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 45s 107ms/step - accuracy: 0.9883 - loss: 0.0361 - val_accuracy: 0.9920 - val_loss: 0.0301
Epoch 13/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 82s 107ms/step - accuracy: 0.9884 - loss: 0.0345 - val_accuracy: 0.9928 - val_loss: 0.0282
Epoch 14/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 82s 106ms/step - accuracy: 0.9896 - loss: 0.0321 - val_accuracy: 0.9920 - val_loss: 0.0285
Epoch 15/15
422/422 ━━━━━━━━━━━━━━━━━━━━ 82s 107ms/step - accuracy: 0.9882 - loss: 0.0360 - val_accuracy: 0.9923 - val_loss: 0.0275
Test loss: 0.023274529725313187
Test accuracy: 0.9919000267982483
```

Enusre that the **paths** for loading data and saving outputs and are correctly set as needed. Below is an example of how to save results, the trained model, and predictions to Google Drive:

```python title="colab_test.ipynb"
import os

# Define the output directory (make sure it exists)
output_dir = './outputs'
os.makedirs(output_dir, exist_ok=True)

# Save test results to a file in the output directory
results_file = os.path.join(output_dir, 'results.txt')
with open(results_file, 'w') as f:
    f.write(f"Test loss: {score[0]}\n")
    f.write(f"Test accuracy: {score[1]}\n")
print(f"Evaluation results saved to {results_file}")

# Save predictions as a NumPy array in the output directory
predictions_file = os.path.join(output_dir, 'predictions.npy')
predictions = model.predict(x_test)
np.save(predictions_file, predictions)
print(f"Predictions saved to {predictions_file}")

# Save the entire model in the native Keras format
model_file = os.path.join(output_dir, 'mnist_model.keras')
model.save(model_file)
print(f"Model saved to {model_file}")

```

Running the above code will generate files in your Google Drive, with output messages like this:

```{ .yaml .no-copy title="Notebook Output" }
Evaluation results saved to ./outputs/results.txt
Predictions saved to ./outputs/predictions.npy
Model saved to ./outputs/mnist_model.keras
313/313 ━━━━━━━━━━━━━━━━━━━━ 4s 14ms/step
```